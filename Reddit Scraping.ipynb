{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Script for running at night or unattended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new script is set to go to r/funny and get the first 100 posts in 'new' section and follow those 100 posts for 24hrs, collecting upvotes, # of comments, and subreddit activity every 30minutes.\n",
    "\n",
    "If automated script below fails, go to 'manual script' section.\n",
    "\n",
    "There are 2 cells in 'manual script', the first one allows you to rerun the automation script and continue where you left off until 48 datapoints per post is stored (since there are 48 30min intervals in 24hrs). Or, if you want to just manually collect data every 30 minutes, run the second cell once every 30 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(client_id='eOqj6Q2BiQQyKQ', client_secret='1hqeAwOvlkY1PYw51ewhbrItd5k', user_agent='Reddit WebScrapping')\n",
    "\n",
    "sub_title = 'funny' #title of subreddit to scrape\n",
    "\n",
    "time2sleep = 1800 #how long the program should sleep before continuing\n",
    "hours = 48 #how long(30 minute intervals) you want to run the script for\n",
    "count = 0 #each count represents 1 interval passing\n",
    "\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "weekday = dt_object.strftime('%A')\n",
    "\n",
    "fname_votes = './{}_votes.csv'.format(weekday)\n",
    "fname_comments = './{}_comments.csv'.format(weekday)\n",
    "fname_online = './{}_online_users.csv'.format(weekday)\n",
    "\n",
    "votes = {} #store post ids and votes. Positive votes and NEGATIVE votes (when post.downs is > 0)\n",
    "totalcomments = {} #store post ids and the corresponding number of comments\n",
    "online = {'timestamp':[],'users_online':[]} #store number of online users and timestamp of subreddit\n",
    "\n",
    "subred = reddit.subreddit(sub_title)\n",
    "\n",
    "#get 100 posts from new\n",
    "sub_posts = subred.new(limit=100)\n",
    "\n",
    "#initialize the storage of the 100 posts that we are tracking\n",
    "for post in sub_posts:\n",
    "    pid = str(post.id)\n",
    "    if pid not in votes: #if new post, add it to the dictionary\n",
    "        votes[pid] = [] #initialize empty list\n",
    "        totalcomments[pid] = [] #initialize empty list\n",
    "    else:#just in case we get some repeating posts\n",
    "        pass\n",
    "\n",
    "while (count <= hours):\n",
    "    \n",
    "    print('Starting 30min interval {}'.format(count))\n",
    "    subred = reddit.subreddit(sub_title)\n",
    "    #every 30minutes store number of online users in the subreddit\n",
    "    online['timestamp'].append(time.time())\n",
    "    online['users_online'].append(subred.active_user_count)\n",
    "    \n",
    "    for id_val in votes.keys():\n",
    "        result = reddit.submission(id=id_val) #go to the specific post\n",
    "        votes[id_val].append(result.ups - result.downs) #get the current number of upvotes and add to the list\n",
    "        totalcomments[id_val].append(result.num_comments) #get number of comments\n",
    "        \n",
    "    #store into csv every 30min interval in case something goes wrong halfway through\n",
    "    upvotes = pd.DataFrame(list(votes.values()),index=votes.keys())\n",
    "    upvotes.to_csv(fname_votes,index_label='id')\n",
    "\n",
    "    comments = pd.DataFrame(list(totalcomments.values()),index=totalcomments.keys())\n",
    "    comments.to_csv(fname_comments,index_label='id')\n",
    "\n",
    "    on = pd.DataFrame(list(online.values()),index=online.keys())\n",
    "    on.to_csv(fname_online,index_label='timestamp')\n",
    "    \n",
    "    if count == hours:\n",
    "        print('finished collecting data')\n",
    "        break\n",
    "    else:\n",
    "        print('sleeping for {} seconds...'.format(time2sleep))\n",
    "        time.sleep(time2sleep)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Continue where you left off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to continue the automated data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "time2sleep = 1800\n",
    "reddit = praw.Reddit(client_id='eOqj6Q2BiQQyKQ', client_secret='1hqeAwOvlkY1PYw51ewhbrItd5k', user_agent='Reddit WebScrapping')\n",
    "\n",
    "sub_title = 'funny' #title of subreddit to scrape\n",
    "\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "weekday = dt_object.strftime('%A')\n",
    "\n",
    "fname_votes = './{}_votes.csv'.format(weekday)\n",
    "df_votes = pd.read_csv(fname_votes)\n",
    "#df_votes.set_index('id',inplace=True)\n",
    "\n",
    "fname_comments = './{}_comments.csv'.format(weekday)\n",
    "df_comments = pd.read_csv(fname_comments)\n",
    "#df_comments.set_index('id',inplace=True)\n",
    "\n",
    "fname_online = './{}_online_users.csv'.format(weekday)\n",
    "df_online = pd.read_csv(fname_online)\n",
    "#df_online.set_index('timestamp',inplace=True)\n",
    "\n",
    "votes = {} #store post ids and votes. Positive votes and NEGATIVE votes (when post.downs is > 0)\n",
    "totalcomments = {} #store post ids and the corresponding number of comments\n",
    "online = {'timestamp':[],'users_online':[]} #store number of online users and timestamp of subreddit\n",
    "\n",
    "#initialize the storage of the 100 posts that we are tracking\n",
    "for pid in df_votes['id']:\n",
    "    if pid not in votes: #if new post, add it to the dictionary\n",
    "        votes[pid] = [] #initialize empty list\n",
    "        totalcomments[pid] = [] #initialize empty list\n",
    "    else:#just in case we get some repeating posts\n",
    "        pass\n",
    "datapoints = 48 - df_votes.shape[1]\n",
    "count = 0\n",
    "while (count <= datapoints):\n",
    "    subred = reddit.subreddit(sub_title)\n",
    "    #get online users\n",
    "    online['timestamp'].append(time.time())\n",
    "    online['users_online'].append(subred.active_user_count)\n",
    "    \n",
    "    #get votes/comments\n",
    "    for id_val in votes.keys():\n",
    "        result = reddit.submission(id=id_val) #go to the specific post\n",
    "        votes[id_val].append(result.ups - result.downs) #get the current number of upvotes and add to the list\n",
    "        totalcomments[id_val].append(result.num_comments) #get number of comments\n",
    "        \n",
    "    #create into dataframe, merge with old, and save to csv\n",
    "    temp = pd.DataFrame(list(votes.values()),index=votes.keys())\n",
    "    #temp.index.names = ['id']\n",
    "    if count == 0:\n",
    "        df_votes.set_index('id',inplace=True)\n",
    "    upvotes = pd.concat([df_votes,temp],axis=1)\n",
    "    upvotes.to_csv(fname_votes,index_label='id')\n",
    "\n",
    "    temp = pd.DataFrame(list(totalcomments.values()),index=totalcomments.keys())\n",
    "    #temp.index.names = ['id']\n",
    "    if count == 0:\n",
    "        df_comments.set_index('id',inplace=True)\n",
    "    comments = pd.concat([df_comments,temp],axis=1)\n",
    "    comments.to_csv(fname_comments, index_label='id')\n",
    "\n",
    "    temp = pd.DataFrame(list(online.values()),index=online.keys())\n",
    "    #temp.index.names = ['timestamp']\n",
    "    if count == 0:\n",
    "        df_online.set_index('timestamp',inplace=True)\n",
    "    on = pd.concat([df_online,temp],axis=1)\n",
    "    on.to_csv(fname_online, index_label='timestamp')\n",
    "    \n",
    "    if count == datapoints:\n",
    "        print('done')\n",
    "        break\n",
    "    else:\n",
    "        print('sleeping...{}'.format(count))\n",
    "        count += 1\n",
    "        time.sleep(time2sleep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Manual data collection every 30min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell below manually every 30minutes to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(client_id='eOqj6Q2BiQQyKQ', client_secret='1hqeAwOvlkY1PYw51ewhbrItd5k', user_agent='Reddit WebScrapping')\n",
    "\n",
    "sub_title = 'funny' #title of subreddit to scrape\n",
    "\n",
    "dt_object = datetime.fromtimestamp(time.time())\n",
    "weekday = dt_object.strftime('%A')\n",
    "\n",
    "fname_votes = './{}_votes.csv'.format(weekday)\n",
    "df_votes = pd.read_csv(fname_votes)\n",
    "#df_votes.set_index('id',inplace=True)\n",
    "\n",
    "fname_comments = './{}_comments.csv'.format(weekday)\n",
    "df_comments = pd.read_csv(fname_comments)\n",
    "#df_comments.set_index('id',inplace=True)\n",
    "\n",
    "fname_online = './{}_online_users.csv'.format(weekday)\n",
    "df_online = pd.read_csv(fname_online)\n",
    "#df_online.set_index('timestamp',inplace=True)\n",
    "\n",
    "votes = {} #store post ids and votes. Positive votes and NEGATIVE votes (when post.downs is > 0)\n",
    "totalcomments = {} #store post ids and the corresponding number of comments\n",
    "online = {'timestamp':[],'users_online':[]} #store number of online users and timestamp of subreddit\n",
    "\n",
    "subred = reddit.subreddit(sub_title)\n",
    "\n",
    "#initialize the storage of the 100 posts that we are tracking\n",
    "for pid in df_votes['id']:\n",
    "    if pid not in votes: #if new post, add it to the dictionary\n",
    "        votes[pid] = [] #initialize empty list\n",
    "        totalcomments[pid] = [] #initialize empty list\n",
    "    else:#just in case we get some repeating posts\n",
    "        pass\n",
    "\n",
    "#get online users\n",
    "online['timestamp'].append(time.time())\n",
    "online['users_online'].append(subred.active_user_count)\n",
    "    \n",
    "#get votes/comments\n",
    "for id_val in votes.keys():\n",
    "    result = reddit.submission(id=id_val) #go to the specific post\n",
    "    votes[id_val].append(result.ups - result.downs) #get the current number of upvotes and add to the list\n",
    "    totalcomments[id_val].append(result.num_comments) #get number of comments\n",
    "        \n",
    "#create into dataframe, merge with old, and save to csv\n",
    "temp = pd.DataFrame(list(votes.values()),index=votes.keys())\n",
    "#temp.index.names = ['id']\n",
    "df_votes.set_index('id',inplace=True)\n",
    "upvotes = pd.concat([df_votes,temp],axis=1)\n",
    "upvotes.to_csv(fname_votes,index_label='id')\n",
    "\n",
    "temp = pd.DataFrame(list(totalcomments.values()),index=totalcomments.keys())\n",
    "#temp.index.names = ['id']\n",
    "df_comments.set_index('id',inplace=True)\n",
    "comments = pd.concat([df_comments,temp],axis=1)\n",
    "comments.to_csv(fname_comments, index_label='id')\n",
    "\n",
    "temp = pd.DataFrame(list(online.values()),index=online.keys())\n",
    "#temp.index.names = ['timestamp']\n",
    "df_online.set_index('timestamp',inplace=True)\n",
    "on = pd.concat([df_online,temp],axis=1)\n",
    "on.to_csv(fname_online, index_label='timestamp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
