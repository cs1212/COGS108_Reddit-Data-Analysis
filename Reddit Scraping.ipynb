{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the 'hours' variable depending on how many datapoints you want to collect.Keep in mind that if you want to collect 5 hours of data, hours=10 because for example at hour4 you get a new post and you want 5 datapoints then you have to run the script for 5 more hours. (The code won't collect new posts after hour5).\n",
    "\n",
    "Change the 'sub_title' variable to whatever subreddit name is.\n",
    "\n",
    "'time2sleep' variable is how many seconds you want the program to go to sleep before continuing data collection. ie if we want data collected every hour then time2sleep=3600 seconds. You still have to keep the program and your computer on and running during the sleeping phase though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(client_id='eOqj6Q2BiQQyKQ', client_secret='1hqeAwOvlkY1PYw51ewhbrItd5k', user_agent='Reddit WebScrapping')\n",
    "\n",
    "sub_title = 'funny' #title of subreddit to scrape\n",
    "time2sleep = 3600 #how long the program should sleep before continuing\n",
    "votes = {} #store post ids and votes. Positive votes and NEGATIVE votes (when post.downs is > 0)\n",
    "totalcomments = {} #store post ids and the corresponding number of comments\n",
    "timestamps = {} #store timestamp of post creation just so we can tell what day/hour\n",
    "online = {'timestamp':[],'users_online':[]} #store number of online users and timestamp of subreddit\n",
    "count = 0 #each count represents 1hr\n",
    "hours = 10 #how long(hours) you want to run the script for\n",
    "subred = reddit.subreddit(sub_title)\n",
    "\n",
    "while (count < hours):\n",
    "    \n",
    "    print('Starting hour {}'.format(count))\n",
    "    \n",
    "    if count < int(hours/2):\n",
    "        sub = reddit.subreddit(sub_title).new(limit=10) #collect first X values in 'new' section\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    online['timestamp'].append(time.time())\n",
    "    online['users_online'].append(subred.active_user_count)\n",
    "    \n",
    "    #first loop through should produce nothing since there is nothing inside post_id\n",
    "    for id_val in votes.keys():\n",
    "        '''\n",
    "        We want to follow each post for 24hrs so at max we need 24 datapoints per post.\n",
    "        A fresh post collected at hour 23 would then need an extra 24hrs to get 24 datapoints\n",
    "        but we also do not want to collect more data from the previous posts at hour1,2, etc.\n",
    "        '''\n",
    "        if len(votes[id_val]) != int(hours/2): #if we do not have 24 datapoints\n",
    "            result = reddit.submission(id=id_val) #go to the specific post\n",
    "            votes[id_val].append(result.ups - result.downs) #get the current number of upvotes and add to the list\n",
    "            totalcomments[id_val].append(result.num_comments) #get number of comments\n",
    "        else:\n",
    "            pass #do nothing if we already collected data from a post for 24hrs\n",
    "        \n",
    "    for post in sub:\n",
    "        pid = str(post.id)\n",
    "        if pid not in votes: #if new post, add it to the dictionary\n",
    "            votes[pid] = [post.ups - post.downs] #get the upvotes of the post\n",
    "            totalcomments[pid] = [post.num_comments] #get number of comments\n",
    "            timestamps[pid] = [post.created_utc] #store the timestamps of each post. still need to convert later.\n",
    "        else:#just in case we get some repeating posts\n",
    "            pass\n",
    "    if count == hours - 1:\n",
    "        print('finished collecting data')\n",
    "        break\n",
    "    else:\n",
    "        print('sleeping...')\n",
    "        time.sleep(time2sleep)\n",
    "        count+=1\n",
    "    \n",
    "upvotes = pd.DataFrame(list(votes.values()),index=votes.keys())\n",
    "upvotes.to_csv('./votes.csv',index_label='id')\n",
    "\n",
    "comments = pd.DataFrame(list(totalcomments.values()),index=totalcomments.keys())\n",
    "comments.to_csv('./comments.csv',index_label='id')\n",
    "\n",
    "times = pd.DataFrame(list(timestamps.values()),index=timestamps.keys())\n",
    "times.to_csv('./post_time.csv',index_label='id')\n",
    "\n",
    "on = pd.DataFrame(list(online.values()),index=online.keys())\n",
    "on.to_csv('./online_users.csv',index_label='timestamp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
